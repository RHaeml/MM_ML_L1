{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "143d428c",
   "metadata": {},
   "source": [
    "# Vorlesungswiederholung\n",
    "Bestimmung einer Funktion zur Vorhersage von Merkmalen (Nominal/Ordinal/Kardinal) unter Verwendung von vollständigen Trainingsdaten (training-set). Nach ihrer Bestimmung wird die Funktion anhand weiterer Daten getestet (testing-set).\n",
    "\n",
    "### Klassifizierung\n",
    "Alle vor dem Lernprozess verfügbaren Daten liegen kategorisiert vor. Jeder Datenpunkt besteht aus Merkmalen (Kovariablen) und einer zugeordneten Kategorie. Ziel der Klassifizierung ist eine Funktion abzuleiten, welche neue Daten der gegebenen Menge von Kategorien zuordnet. Die zur Bestimmung der Funktion verwendeten Trainingsdaten bestehen je aus einem Eingangsvektor und dem gewünschten Rückgabewert (Überwachungssignal). Eine weitere Generalisierung hat zum Ziel vollständig unbekannte Datenpunkte einer passenden Kategorie zuzuordnen. \n",
    "\n",
    "### Regression\n",
    "Anstelle den Daten zugeordnete Kategorien liegen kontinuierliche Ausgangsgrößen (abhängige Variablen) vor. Ziel ist die Erstellung einer Schätzfunktion mit kontinuierlichem Wertebereich. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003a0fb",
   "metadata": {},
   "source": [
    "### Lineare Modelle\n",
    "Umfassen Methoden zur Datenmodellierung mittels linearer Regression. Der vorhergesagte Zielwert $y$ (target) wird durch eine Linearkombination aller gegebenen Merkmale (features) berechnet:\n",
    "\n",
    "$$y(w,x)=w_{0}+w_{1}x_{1}+...+w_{n}x_{n}$$\n",
    "\n",
    "Vektor $\\vec{x}=(x_{1},..,x_{n})$ enthält die Merkmalswerte und Vektor $\\vec{w}=(w_{0},w_{1},..,w_{n})$ enthält die Regressionskoeffizienten. Die Schätzung der Regressionskoeffizienten setzt die Unabhängigkeit aller Merkmale in $\\vec{x}$ voraus. Ist diese Voraussetzung nicht erfüllt, ist eine hohe Modellvarianz die Folge (Schon durch kleine Schwankungen der Zielwerte, z.B. durch zufällige Fehler, entstehen große Abweichungen innerhalb der Regressionskoeffizienten.)\n",
    "\n",
    "Das Beispiel im folgenden Bild enthält beobachtete Punkte und ihre Approximation mit einem linearen Modell. Die Approximation erfolgt durch die Minimierung der RSS (residual sum of sqaures) zwischen den vorhandenen Daten und den vorhergesagten Daten. (Klasse: sklearn.linear_model.LinearRegression) \n",
    "<img src=\"Lineares_Modell.png\" style=\"width: 800px; height: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5f25a",
   "metadata": {},
   "source": [
    "### Polynomiale Regression nichtlinearer Daten\n",
    "Lineare Modelle (Schätzer) lassen sich durch polynomiale Regression auch auf nichtlineare Abbildungen von Daten anwenden. Dabei wird der Merkmalbereich um  interagierende oder potenzierte Merkmale erweitert, denen jeweils ein neuer linearer Regressionskoeffizient zugeordnet wird.\n",
    "\n",
    "Ein ebenes lineares Modell könnte folgendermaßen aussehen:\n",
    "\n",
    "$$y(w,x)=w_{0}+w_{1}x_{1}+w_{2}x_{2}$$\n",
    "\n",
    "Durch eine entsprechende lineare Modellerweiterung (Transformierung) nimmt das Modell die Form eines Paraboloids an:\n",
    "\n",
    "$$y(w,x)=w_{0}+w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{1}x_{2}+w_{4}x_{1}^{2}+w_{5}x_{2}^{2}$$\n",
    "\n",
    "Eine Neubenennung der Merkmale verdeutlicht, dass sich dieses Modell mit den Methoden der linearen Modellierung an die Trainingsdaten anpassen lässt:\n",
    "\n",
    "$$y(w,x)=w_{0}+w_{1}z_{1}+w_{2}z_{2}+w_{3}z_{3}+w_{4}z_{4}+w_{5}z_{5},$$\n",
    "\n",
    "mit$$\\vec{z}=(x_{1},x_{2},x_{1}x_{2},x_{1}^{2},x_{2}^{2}).$$\n",
    "\n",
    "Im folgenden Bild ist die polynomiale Regression eindimensionaler Daten in Abhängigkeit des Erweiterungsgrades dargestellt. \n",
    "(Klassen: sklearn.preprocessing.PolynomialFeatures und sklearn.linear_model.LinearRegression) \n",
    "<img src=\"Polynomiale_Modelle.png\" style=\"width: 800px; height: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bcaf92",
   "metadata": {},
   "source": [
    "___\n",
    "# Interaktiver Teil #\n",
    "**Ziel:** Erstellen eines Modells, welches gut angepasst auf die Daten ist.\n",
    "\n",
    "Zur Verdeutlichung des Einflusses, welchen polynomial erzeugte Merkmale und ihr höchster Grad auf ein berechnetes Modell haben, wird zunächst einmal eine nichtpolynomiale lineare Regression von sinusförmig verteilten Daten durchgeführt.\n",
    "\n",
    "Zu Beginn werden die der Modellierung zugrunde liegenden Daten erzeugt. Das Merkmal X enthält zufällige Punkte (samples) zwischen $0$ und $2\\pi$. Die abhängige Variable Y erhält man durch die Anwendung der Sinusfunktion auf die Definitionsmenge. Die erzeugten Daten sind somit unsortiert, aber haben eine feste Reihenfolge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4c0ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "X = np.arange(start = 0, stop = 2*np.pi, step = 0.1)\n",
    "Y_sin = np.sin(X)\n",
    "\n",
    "data_of_sin = pd.DataFrame({'X':X, 'Y': Y_sin})\n",
    "data_of_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e07a9",
   "metadata": {},
   "source": [
    "Im Plot sehen die Daten folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67500d47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ploten der Daten\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.title('Elements given by the sinus function')\n",
    "plt.scatter(data_of_sin[\"X\"], data_of_sin['Y'],label='target_values',s=5)\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('X') #\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b23700",
   "metadata": {},
   "source": [
    "Im Folgenden wird zu allen Daten eine normalverteilte Streuung ergänzt, wie sie auch bei real gemessenen Daten auftritt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227ee9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_factor = 0.2 #Scaling of the random noise\n",
    "Y_sin_rand = np.sin(X)+ random_factor * np.random.randn(len(X))\n",
    "data_of_sin = pd.DataFrame({'X':X, 'Y': Y_sin_rand})\n",
    "\n",
    "# Ploten der Daten\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.title('Elements given by the randomized sinus function')\n",
    "plt.scatter(data_of_sin[\"X\"], data_of_sin['Y'],s=5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dabca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_values = data_of_sin[\"X\"].to_numpy() #Convert to numpy\n",
    "y_values = data_of_sin['Y'].to_numpy() #Convert to numpy\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ada62",
   "metadata": {},
   "source": [
    "Die Unterteilten Trainingsdaten werden noch einmal geplottet. Sie sehen nun so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ec1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.title('Training- vs. Testdata')\n",
    "plt.scatter(x_train, y_train, label='Training data',s=5, c='blue')\n",
    "plt.scatter(x_test, y_test, label='Test data',s=5, c='red')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d7619",
   "metadata": {},
   "source": [
    "Um die Daten zum überwachten Lernen einzusetzen müssen sie zunächst in Trainings- und Testdaten unterteilt und in Matrixform (n_samples $\\times$ n_features/n_targets) (hier für x_train (50 $\\times$ 1)) gebracht werden. In diesen Beispiel liegen sie jedoch als (n_samples,) vor. Sie werden daher umgewendelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7317859",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vorher ', x_train.shape)\n",
    "x_train = x_train[:,np.newaxis]\n",
    "x_test = x_test[:,np.newaxis]\n",
    "y_train = y_train[:,np.newaxis]\n",
    "y_test = y_test[:,np.newaxis]\n",
    "print('Nachher ', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217c429",
   "metadata": {},
   "source": [
    "Anhand der Trainigsdaten führen wir anschließend eine einfache Regression durch und fertigen ein lineares Modell an. Dafür verwenden wir das Modul \"sklearn.linear_model\" und erzeugen eine Instanz vom Typ \"LinearRegression\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.LinearRegression() #Definition of the model\n",
    "reg.fit(x_train,y_train) #Fit of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d4eab",
   "metadata": {},
   "source": [
    "Das untrainierte Modell kann nun an die Trainingsdaten angepasst werden. Das Resultat ist eine Schätzfunktion mit fixen Regressionskoeffienten und wird \"predictor\" genannt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14055f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{string:<25} {n_feat}'.format(string = 'Anzahl der Merkmale', n_feat = reg.n_features_in_))\n",
    "print('{string:<25} {coef}'.format(string = 'Koeffizienten', coef = reg.coef_))\n",
    "print('{string:<25} {inter}'.format(string = 'Achsenabschnitt', inter = reg.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f297ab9",
   "metadata": {},
   "source": [
    "Mit dem trainierten Modell können nun anhand der Testdaten Vorhersagen gemacht werden, welche mit den wahren Zielwerten (testing_targets) verglichen werden können. Zur Einschätzung der Vorhersagekraft werden die Ergebnisse geplottet und die mittlere quadratische Abweichung (MSE, kleiner ist besser) zwischen den vorhergesagten Zielwerten und den wahren Zielwerten sowie das Bestimmtheitsmaß $R^2$ (1 ist optimal) berechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45bb5e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE, r2_score\n",
    "\n",
    "predicted_targets = reg.predict(x_test) #Model predict x_test\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Prediction with testing data')\n",
    "\n",
    "plt.scatter(x_train,reg.predict(x_train),label='Model prediction training data', color='gray' , marker = 'x')\n",
    "plt.scatter(x_train,y_train,label='Training data', color='gray', marker = 'o')\n",
    "\n",
    "plt.scatter(x_test,predicted_targets,label='Model prediction test data', color='red', marker = 'x')\n",
    "plt.scatter(x_test,y_test,label='test data', color='red', marker = 'o')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "print('{string:<25} {MSE:.2f}'.format(string = 'MSE', MSE = MSE(y_test, predicted_targets)))\n",
    "print('{string:<25} {R2:.2f}'.format(string = 'R2', R2 = r2_score(y_test, predicted_targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d427a94",
   "metadata": {},
   "source": [
    "Die Vorhersagekraft des nichtpolynomial erzeugten Modells ist nicht hoch. Um die Vorhersagekraft zu optimieren werden im Folgenden polynome Merkmale mittels eines Transformators ergänzt und die Fähigkeit zur Schätzung der Zielwerte in Abhängigkeit vom Erweiterungsgrad überwacht.\n",
    "***\n",
    "Der Transformator erzeugt mit den Daten der vorherigen Trainingsmatrix eine neue Trainingsmatrix, welche zusätzlich zu den alten Merkmalen neu errechnete Merkmale höheren Grades enthält. Der höchste Grad wird im Vorfeld bei der Instanziierung des neuen Transformationsobjekts definiert (es ist aber auch möglich ausschließlich Interaktionen der Merkmale zu berücksichtigen). \n",
    "\n",
    "Mit den neuen Trainingsdaten kann anschließend ein einfaches lineares Modell mit einer linearen Regression an die Trainingsdaten angepasst werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c995e0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly2 = PolynomialFeatures(degree=2, include_bias=False) #Create the transformer\n",
    "poly2.fit(x_train) \n",
    "poly2_x_train = poly2.transform(x_train) \n",
    "poly2_x_test = poly2.transform(x_test) \n",
    "\n",
    "print('{string:<25} {n_feat}'.format(string = 'Merkmale vorher', n_feat = poly2.n_features_in_))\n",
    "print('{string:<25} {n_feat}\\n'.format(string = 'Merkmale nachher', n_feat = poly2.n_output_features_))\n",
    "\n",
    "print(poly2_x_train[-10:]) #10 data points are output for illustration purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd1fb6",
   "metadata": {},
   "source": [
    "Nun kann wie bereits oben vorgeführt eine neue lineare Regression anhand der gewonnenen polynomen Traningsdaten durchgeführt werden. Anschließend benutzen wir das neue Modell um Vorhersagen zu machen und diese mit den ebenfalls polynomisierten Testdaten zu vergleichen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ac47e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "poly2_reg = linear_model.LinearRegression()\n",
    "poly2_reg.fit(poly2_x_train, y_train) #Training\n",
    "\n",
    "print('{string:<25} {n_feat}'.format(string = 'Anzahl Regr. Merkmale', n_feat = poly2_reg.n_features_in_))\n",
    "print('{string:<25} {coef}'.format(string = 'Regr. Koeffizienten', coef = poly2_reg.coef_))\n",
    "print('{string:<25} {inter}\\n'.format(string = 'Achsenabschnitt', inter = poly2_reg.intercept_))\n",
    "\n",
    "# Prediction\n",
    "predicted_targets = poly2_reg.predict(poly2_x_test)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Testing data, degree 2')\n",
    "\n",
    "plt.scatter(x_train, poly2_reg.predict(poly2_x_train),label='Model prediction training data', color='gray' , marker = 'x')\n",
    "plt.scatter(x_train,y_train,label='Training data', color='gray', marker = 'o')\n",
    "\n",
    "plt.scatter(x_test, predicted_targets,label='Model prediction test data', color='red', marker = 'x')\n",
    "plt.scatter(x_test,y_test,label='test data', color='red', marker = 'o')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "print('{string:<25} {MSE:.2f}'.format(string = 'MSE', MSE = MSE(y_test, predicted_targets)))\n",
    "print('{string:<25} {R2:.2f}'.format(string = 'R2', R2 = r2_score(y_test, predicted_targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa49334",
   "metadata": {},
   "source": [
    "Das angepasste Modell mit Grad 2 hat nun einen quadratischen Verlauf, schneidet aber bei der Validierung mit den Testdaten nicht in jedem Fall besser ab (Erkennbar an der Anpassungsgüte). Das liegt daran, dass das Modell ausschließlich auf den Trainingsdaten beruht.\n",
    "***\n",
    "Nun soll betrachtet werden, was passiert, wenn ein Modell eines höheren Polynomgrades verwendet werden soll. Hierzu fassen wir die durchgeführten Schritte zu einer Funktion zusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0dd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_reg_model(degree, x_train, y_train, x_test, y_test):\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False) #Erzeugen des Transformators\n",
    "    poly.fit(x_train)\n",
    "    \n",
    "    poly_x_train = poly.transform(x_train) \n",
    "    poly_x_test = poly.transform(x_test) \n",
    "    \n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(poly_x_train, y_train)\n",
    "    \n",
    "    predicted_y_test = reg.predict(poly_x_test)\n",
    "    predicted_y_train = reg.predict(poly_x_train)\n",
    "    \n",
    "    MSE_Train = MSE(predicted_y_train, y_train)\n",
    "    MSE_Test = MSE(predicted_y_test, y_test)\n",
    "    \n",
    "    return(MSE_Train, MSE_Test, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5bc376",
   "metadata": {},
   "source": [
    "Über degree wird der Grad des Ansatzes für das Polynom festgelegt. Hierzu wird die Funktion mehrfach durchlaufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for degree in range(20):\n",
    "    if degree == 0: #No transformation possile if degree == 0\n",
    "        MSE_train, MSE_test, _ = fit_reg_model(degree + 1, x_train, y_train, x_test, y_test)\n",
    "        plt.scatter(degree, MSE_train, c='red', label = 'training')\n",
    "        plt.scatter(degree, MSE_test, c='blue',label = 'test')\n",
    "    else:\n",
    "        MSE_train, MSE_test, _= fit_reg_model(degree + 1, x_train, y_train, x_test, y_test)\n",
    "        plt.scatter(degree, MSE_train, c='red')\n",
    "        plt.scatter(degree, MSE_test, c='blue')\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fdc594",
   "metadata": {},
   "source": [
    "Wie der MSE Wert und die Anpassungsgüte $R^2$ zeigen, wird der Anpassungsfehler bei den Trainingsdaten mit steigendem Grad immer kleiner, aber die Vorhersagekraft bei den Testdaten nimmt mit steigendem Grad irgendwann wieder ab. \n",
    "\n",
    "### Bias vs. Variance\n",
    "\n",
    "Im letzten Beispiel sieht man sehr gut, dass mit steigender Modellkomplexität die Anpassung an das Modell optimaler wird (kleinerer Bias), aber im Gegensatz dazu die Vorhersagekraft stark abnimmt (höhere Modellvarianz). Bei zu geringer Komplexität ist die Anpassung des Modells an die Trainingsdaten schlecht, woraus auch eine schlechte Vorhersagekraft resultiert. Es gilt daher ein ausgewogenes Maß bei der Annäherung an die Trainingsdaten während der Modellierung zu finden.\n",
    "\n",
    "<img src=\"Bias_Variance_Trade Off.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e06d60",
   "metadata": {},
   "source": [
    "Das Modell wir die mit höheren Polynomgrad immer komplexer. Findet eine starke Verbesserung des Modells statt. Bei hohen Polynomgraden nimmt die Performance der Modelle stark ab (bzw. der MSE nimmt stark zu). Modelle mit niedrigen Polynomgrad sind nicht komplex genug, um die gegebenen Daten ausreichend genau fitten zu können. Die Modell zeigen Unterfitting. Modelle mit hohen Polynomgrad zeigen eine Überanpassung an die Daten und überfitten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_underfit = 1\n",
    "degree_good = 5\n",
    "degree_overfit = 20\n",
    "\n",
    "_, _, underfit_model = fit_reg_model(degree_underfit, x_train, y_train, x_test, y_test)\n",
    "_, _, good_model = fit_reg_model(degree_good, x_train, y_train, x_test, y_test)\n",
    "_, _, overfit_model = fit_reg_model(degree_overfit, x_train, y_train, x_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Overfit vs. Underfit')\n",
    "\n",
    "poly = PolynomialFeatures(degree=degree_underfit, include_bias=False)\n",
    "poly.fit(x_train)\n",
    "plt.plot(X, underfit_model.predict(poly.transform(X[:,np.newaxis])), label = 'Underfit', color = 'blue', linewidth=3)\n",
    "\n",
    "poly = PolynomialFeatures(degree=degree_good, include_bias=False)\n",
    "poly.fit(x_train)\n",
    "plt.plot(X, good_model.predict(poly.transform(X[:,np.newaxis])), label = 'Good', color = 'green', linewidth=3)\n",
    "\n",
    "poly = PolynomialFeatures(degree=degree_overfit, include_bias=False)\n",
    "poly.fit(x_train)\n",
    "plt.plot(X, overfit_model.predict(poly.transform(X[:,np.newaxis])), label = 'Overfit', color = 'red', linewidth=3)\n",
    "\n",
    "poly = PolynomialFeatures(degree=degree_overfit, include_bias=False)\n",
    "poly.fit(x_train)\n",
    "plt.plot(X, Y_sin, label = 'Target', linewidth=5, color = 'black')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
