{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9605bc",
   "metadata": {},
   "source": [
    "# Vorlesungswiederholung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687cdd7e",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "$$\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$$\n",
    "\n",
    "Die Schwierigkeit besteht darin Alpha so zu optimieren, dass das Modell die zugrunde liegenden Daten mit optimalem \"Grad der Generalisierung\" und Varianz wiedergibt.\n",
    "Als Kriterium kommen der resultierende MSE-Wert (mean squared error) oder sogenannte Informationskriterien wie das Akaike information criterion (AIC) oder das Bayes information criterion (BIC) in Frage. \n",
    "\n",
    "Im Folgenden Beispiel wird Alpha anhand des MSE-Werts optimiert. Mit definierten Werten von Alpha werden Modelle bestimmt. Das beste Modell wird mit k-facher Cross-Validierung ausgewählt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42218ec3",
   "metadata": {},
   "source": [
    "### Ridge\n",
    "\n",
    "Bei der Ridge Regression geht der mit $\\alpha$ skalierte Fehlerterm quadratisch in das im Abschnitt \"Lasso\" vorgestellte Minimierungsproblem ein ($L_2$-Regularisierung):\n",
    "\n",
    "$$\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_2 ^ 2}$$\n",
    "\n",
    "Ziel ist es einen Kompromiss zwischen Bias und Varianz zu finden. Mit einem hohen $\\alpha$ kann Overfitting vermieden werden. Wird $\\alpha$ zu groß gewählt, führt dies zu Underfitting (hoher Bias), da die Modellparameter dazu tendieren zu klein zu werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19769215",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "\n",
    "Der Elastic Net Algorithmus führt die Regression mit einer Kombination aus $L_1$- und $L_2$-Regularisierung durch. Elasic Net kann genau wie Ridge angewendet werden, sollten mehrere Merkmale miteinander korreliert sein. Das Minimalisierungsproblem lautet folgendermaßen:\n",
    "\n",
    "$$\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2}$$\n",
    "\n",
    "Optimiert werden müssen die Parameter für $\\alpha$ und $\\rho$, auch $l1_{ratio}$ genannt. $\\rho$ liegt zwischen 0 und 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc25801",
   "metadata": {},
   "source": [
    "### K-fache Cross-Validierung\n",
    "\n",
    "Um Overfitting zu erkennen gibt es die Strategie der Cross-Validierung. Damit soll eine möglichst hohe Generalisierung sichergestellt werden. Jede Art von Wissen über die Daten kann schon bei der Selektion von sogenannten \"Hyperparametern\" (externe Konfigurationsvariablen) die Modellierung beeinflussen. Um diesen Effekt zu minimieren wird das Trainings-Set unterteilt und ein Validierungs-Set erzeugt. Mit dem Validierungs-Set erfolgt eine Vorbewertung des Modells und somit der Hyperparameter. Nach der Modellierung wird das Modell wie gehabt mit den Test-Daten überprüft. \n",
    "\n",
    "Durch die weitere Unterteilung senkt sich aber wiederum die Anzahl der verfügbaren Stichproben und das Modell könnte an Aussagekraft verlieren. Die Auswahl der Daten hätte einen zufälligen Einfluss auf das Modell. \n",
    "\n",
    "Bei der k-fachen Cross-Validierung wird dieses Problem umgangen, indem die Trainingsdaten in k kleinere Datensätze unterteilt werden. Anschließend werden k Schritte durchlaufen, welche immer den gleichen Ablauf haben: Ein Modell wird anhand von k-1 Trainingdatensätzen erstellt und mit dem übrigen Datensatz validiert (seine Performance bestimmt). Der Durchschnitt aller k Validierungen ergibt die Gesamtperformance. \n",
    "\n",
    "<img src=\"k-fache_Cross_Validierung.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2c20e",
   "metadata": {},
   "source": [
    "# Interaktiver Teil\n",
    "**Ziel**: Erstellung eines Modells zur Vorhersage der Zugfestigkeit\n",
    "\n",
    "<img src=\"TrainingValidationPlan.png\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c513cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(r\"Luo_Q_2023.1_Vereinfacht.csv\", sep=';', decimal='.')\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa4032",
   "metadata": {},
   "source": [
    "Wir sehen uns die Spalten der Daten einmal vollständig an und fassen sie in die Unterschiedlichen Kategorien zusammen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "manufacturing_parameter = ['Power', 'Speed']\n",
    "\n",
    "mechanical_parameter = ['Strength_Mean', 'Strength_Std', 'Yield_mean', 'Yield_Std', 'Uniform_Mean', 'Uniform_Std',\n",
    "                        'Ductility_Mean', 'Ductility_Std', 'Youngs_Modulus_Mean', 'Youngs_Modulus_Std']\n",
    "\n",
    "std_parameter = ['Avizo_Porosity_Std', 'Archimedes_porosity_Std','Diameter_Avizo_Std', 'Sphericity_Avizo_Std', \n",
    "                 'Roughness_Avg_Std', 'Roughness_RMS_Std', 'Grain_diameter_Std', 'Grain_aspect_Std', 'Vicker_Std',\n",
    "                'Strength_Std', 'Yield_Std', 'Uniform_Std', 'Ductility_Std', 'Youngs_Modulus_Std']\n",
    "\n",
    "pore_parameter = ['Avizo_Porosity_Mean', 'Archimedes_porosity_Mean', 'Archimedes_porosity_Std',\n",
    "                  'Pore_quantity_Sum', 'Diameter_Avizo_Mean', 'Diameter_Avizo_Std', 'Projected_Pore_area_Avg', \n",
    "                  'Projected_Pore_area_Max', 'Projected_Pore_area_Sum', 'Sphericity_Avizo_Mean', 'Sphericity_Avizo_Std', \n",
    "                  'Roughness_Avg_Mean', 'Roughness_Avg_Std', 'Roughness_RMS_Mean', 'Roughness_RMS_Std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac601234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate train and test sets\n",
    "x = data.drop(labels=(manufacturing_parameter+mechanical_parameter+std_parameter), axis=1)\n",
    "y = data['Strength_Mean']\n",
    "\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f032604",
   "metadata": {},
   "source": [
    "Hier werden unterschiedliche Zusammenhänge geplottet. Bei einigen Parameter sind klare lineare Zusammenhänge zu erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x = data['Strength_Mean'], y = data['Archimedes_porosity_Std']) #% Change Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ea2fa",
   "metadata": {},
   "source": [
    "Nicht alle Werte wurden ermittelt. Um diese dennoch zu verwenden, werden diese auf den Mittelwert gesetzt. Bei der Verwendung einer Regularisierung wie Lasso ($L_1$), Ridge ($L_2$) oder beide gleichzeitig ElasticNet müssen die Daten normalisiert werden. Die hier durchgeführte Datenoperation skaliert die Daten so, dass sie einen Mittelwert von 0 und Standardabweichung von 1 erreichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4213e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Handling missing values by filling them with the mean of each column\n",
    "x_without_NaN = x.fillna(x.mean()) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_without_NaN)\n",
    "x_norm = scaler.transform(x_without_NaN)\n",
    "\n",
    "print(x_norm.mean(axis=0))\n",
    "print(x_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_np = y.to_numpy() #Convert to numpy\n",
    "print(y_np.shape)\n",
    "y_np = y_np[:,np.newaxis]\n",
    "print(y_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e825f",
   "metadata": {},
   "source": [
    "Es wird eine Anpasung eines Modelles mit Hilfe einer Kreuzvalidierung durchgeführt. Hierfür stehen unterschiedliche Modelle zur Verfügung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f22da",
   "metadata": {},
   "source": [
    "Parameter für Lasso, alpha ist die $L_1$-Regularisierung\n",
    "\n",
    "```\n",
    "sklearn.linear_model.Lasso(alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "```\n",
    "\n",
    "---\n",
    "Parameter für Ridge, alpha ist die $L_2$-Regularisierung\n",
    "```\n",
    "sklearn.linear_model.Ridge(alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=None, tol=0.0001, solver='auto', positive=False, random_state=None)\n",
    "```\n",
    "\n",
    "---\n",
    "Parameter für Elastic Net, alpha ist die Gesamtregulierung aus $l1_{ratio} * L_1 + L_2$\n",
    "```\n",
    "sklearn.linear_model.ElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "plt.figure()\n",
    "for fold, (idx_train, idx_test) in enumerate(kf.split(x_norm)):\n",
    "    x_train = x_norm[idx_train]\n",
    "    y_train = y_np[idx_train]\n",
    "    x_test = x_norm[idx_test]\n",
    "    y_test = y_np[idx_test]\n",
    "    \n",
    "    model = LinearRegression() #% Change model\n",
    "    model.fit(x_train,y_train)\n",
    "    \n",
    "    y_predict = model.predict(x_test)\n",
    "    score = MSE(y_test, y_predict)\n",
    "    plt.scatter(x = y_test, y = y_predict, label = str(fold))\n",
    "    \n",
    "    scores.append(score)\n",
    "    models.append(model)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Real value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "\n",
    "print(f'Scores of each fold {scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = x_without_NaN.keys()\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "for fold, model in enumerate(models):\n",
    "    plt.plot(np.squeeze(model.coef_), label = str(fold))\n",
    "plt.xticks(range(len(keys)), keys, rotation='vertical')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
